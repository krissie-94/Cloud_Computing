# Introduction

## Apache Kafka is a distributed data store optimized for ingesting and processing streaming data in real-time. 
## Streaming data is data that is continuously generated by thousands of data sources, which typically send the data records in simultaneously. 
## A streaming platform needs to handle this constant influx of data, and process the data sequentially and incrementally.

Kafka provides three main functions to its users:
- Publish and subscribe to streams of records
- Effectively store streams of records in the order in which records were generated
- Process streams of records in real time

### Kafka is primarily used to build real-time streaming data pipelines and applications that adapt to the data streams.
### It combines messaging, storage, and stream processing to allow storage and analysis of both historical and real-time data.
### We use Apache Kafka in a Linux environment in GCP Platform and reveals how events are consumed and produced using a Kafka-Python.
### The following diagram illustrates the Kafka ecosystem we’re going to set up.

![image](https://user-images.githubusercontent.com/81246356/206632796-1013c6bc-c944-49a9-80b2-581750ba3240.png)


# Spark Streaming

### Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams.

![image](https://user-images.githubusercontent.com/81246356/206632945-0dc23f47-b0ae-4493-9f69-ad937a611cc3.png)


### Internally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.
![image](https://user-images.githubusercontent.com/81246356/206633011-dd4d858c-9530-48dc-8c3c-62fd763889fd.png)


# Installing Spark
-  Download spark-2.3.2 to the local machine:
```
$ wget https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz
```
![img-1 install spark](https://user-images.githubusercontent.com/81246356/207105022-dbe3d08a-8b01-4663-bad2-0e1a8cda8755.jpg)

- Unpack
```
$ tar -xvf spark-3.3.1-bin-hadoop3.tgz
```
![img-2 install spark](https://user-images.githubusercontent.com/81246356/207105056-3d2d329c-37fa-45a7-abf5-e8be5e8db3dc.jpg)

- create a soft link:
```
ln -s /home/krisvesselee18/spark-3.3.1-bin-hadoop3 /home/krisvesselee18/spark
```
# Set Spark Environment Variable

## Add Entry to bashrc file
```
export SPARK_HOME=/home/dpandey/spark

export PATH=$SPARK_HOME/bin:$PATH

export PATH=$SPARK_HOME/sbin:$PATH
```

## Load bashrc file
```
source ~/.bashrc
```
# Install Java
```
sudo apt-get update
sudo apt install default-jdk
java --version
update-alternatives --list java
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/bin/java
```

# Verify Installation
```
pyspark
```
![img-5 install spark](https://user-images.githubusercontent.com/81246356/207106717-e7a83308-70a4-4beb-81f9-5aa79e3429b0.jpg)


### Start the master on local machine
- start-master.sh
- URL: http://34.70.113.82:8080/
![img-9 start master](https://user-images.githubusercontent.com/81246356/207110982-8b709593-5f0b-4d8e-9469-839c5899cf91.jpg)

![img-6 install spark](https://user-images.githubusercontent.com/81246356/207111037-644725a0-6da8-4886-b2a5-441ba9417ffe.jpg)
### Start worker
- start-slave.sh spark://34.70.113.82:7077
- browse URL: http://34.70.113.82:8080/

![img-7 install spark-slave](https://user-images.githubusercontent.com/81246356/207111274-aeceac67-e4d2-47bd-854f-833c545fad6e.jpg)

### RUn Word Count example in Python

- Open Terminal (1)
```
nc -lk 9999
```
![run nc 9999](https://user-images.githubusercontent.com/81246356/207112289-d94ca2d3-3d10-40ab-8613-e953809ad2a9.jpg)

- Open Terminal (2)
```
cd spark
./bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost 9999
```
### Expected Ouput

![word-count output](https://user-images.githubusercontent.com/81246356/207112846-d50065da-d466-489a-8a0a-8191b5895eff.jpg)

# Starting Kafka
- Downlaod kafka which is available at https://kafka.apache.org/downloads
```
$ wget https://downloads.apache.org/kafka/3.3.1/kafka_2.12-3.3.1.tgz

$ tar -xvf kafka_2.12-3.3.1.tgz
```
![img-10 download kafka](https://user-images.githubusercontent.com/81246356/207121824-9dfabf2d-9522-45f7-8502-fd512b9570f2.jpg)

### Start Kafka Zookeeper Open new terminal - keep it open
```
$ cd kafka_2.12-3.3.1/

$ bin/zookeeper-server-start.sh config/zookeeper.properties
```

![img-11 cd zookeeper](https://user-images.githubusercontent.com/81246356/207122013-57c11919-5745-48ce-a95c-b00e49058008.jpg)

### Open a new Terminal -keep it open
```
$ cd kafka_2.12-3.3.1/

$ bin/kafka-server-start.sh config/server.properties
```
![img-12 cd server](https://user-images.githubusercontent.com/81246356/207122345-048e3ce4-b3c3-447d-983d-ce454624b380.jpg)

### Open another new terminal, Create Kafaka topics

```
$ bin/kafka-topics.sh --create --topic input_recommend_product --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1
$ bin/kafka-topics.sh --bootstrap-server localhost:9092 --list
```
![kafka topics](https://user-images.githubusercontent.com/81246356/207126223-1f621ff3-4387-4786-baf3-ab673e0c93ce.jpg)

# Part Three: Event Processing on Apache Spark (PySpark)

## Setup Spark
```
$ sudo apt install python3-pip
$ pip3 install msgpack

$ pip3 install kafka-python

if pip3 command not found,

$ sudo apt install python3-pip

$ wget https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8-assembly_2.11/2.3.2/spark-streaming-kafka-0-8-assembly_2.11-2.3.2.jar
```
### Create and Submit the park Application
```
$ cd

$ vi processor.py
```
![kafka produce py](https://user-images.githubusercontent.com/81246356/207144726-3a9e1064-9594-45b4-8562-4b322064324c.jpg)

```
$ cd

$ vi consumer.py 
```

![kafka consumer py](https://user-images.githubusercontent.com/81246356/207146044-1cc1afcc-4b21-408c-afc9-e37be3b19f4f.jpg)

/bin/python3 consumer.py

Run (Producer Terminal):
![run producer py](https://user-images.githubusercontent.com/81246356/207145424-9903397c-7477-4d5d-8894-9cfea8ffb23f.jpg)

Run (Consumer Terminal):
### Create and Submit the park Application
- Create pyspark_script directory: mkdir pyspark_script
- cd pyspark_script
- Download spark application file from this link: https://github.com/divyapandey03/Cloud-Computing/blob/main/Apache%20Kafka%20%2B%20Spark%20Streaming%20%2B%20PySpark/spark_processor.py
- Download Spark Streaming’s Kafka libraries: wget https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8-assembly_2.11/2.3.2/spark-streaming-kafka-0-8-assembly_2.11-2.3.2.jar

# Submit the spark Application:
```
spark-submit --jars /home/dpandey/pyspark_script/spark-streaming-kafka-0-8-assembly_2.11-2.3.2.jar --master spark:34.70.113.82:7077 --deploy-mode client /home/dpandey/spark_processor.py
```
Spark:

(! https://kontext.tech/article/978/install-hadoop-332-in-wsl-on-windows)

https://kontext.tech/article/1044/install-spark-321-on-linux-or-wsl

https://spark.apache.org/docs/latest/api/python/index.html

Spark Streaming:

https://spark.apache.org/docs/latest/streaming-programming-guide.html

https://spark.apache.org/docs/latest/index.html#running-the-examples-and-shell

https://stackoverflow.com/questions/61891762/spark-3-x-integration-with-kafka-in-python

Kafka:

https://towardsdatascience.com/connecting-the-dots-python-spark-and-kafka-19e6beba6404

https://docs.confluent.io/kafka-clients/python/current/overview.html#ak-python

kafka jar files:

https://mvnrepository.com/artifact/org.apache.spark/spark-streaming-kafka-0-10_2.12/3.3.1

https://mvnrepository.com/artifact/org.apache.spark/spark-sql-kafka-0-10

Spark python kafka intergration:

https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html

https://spark.apache.org/docs/latest/index.html#running-the-examples-and-shell

https://spark.apache.org/docs/latest/submitting-applications.html

https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html

Spark API interface:

https://spark.apache.org/docs/latest/api/python/reference/pyspark.streaming.html

# Project Presentation with Google Slides:
[ Kafka + Spark Streaming + PySpark]https://docs.google.com/presentation/d/1EIGGI8IB0sp9BXnBCkNV5OwEkzLs2Jpi8_oDn7k5AEw/edit?usp=sharing
