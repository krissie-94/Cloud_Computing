# Introduction

## Apache Kafka is a distributed data store optimized for ingesting and processing streaming data in real-time. 
## Streaming data is data that is continuously generated by thousands of data sources, which typically send the data records in simultaneously. 
## A streaming platform needs to handle this constant influx of data, and process the data sequentially and incrementally.

Kafka provides three main functions to its users:
- Publish and subscribe to streams of records
- Effectively store streams of records in the order in which records were generated
- Process streams of records in real time

### Kafka is primarily used to build real-time streaming data pipelines and applications that adapt to the data streams.
### It combines messaging, storage, and stream processing to allow storage and analysis of both historical and real-time data.
### We use Apache Kafka in a Linux environment in GCP Platform and reveals how events are consumed and produced using a Kafka-Python.
### The following diagram illustrates the Kafka ecosystem we’re going to set up.

![image](https://user-images.githubusercontent.com/81246356/206632796-1013c6bc-c944-49a9-80b2-581750ba3240.png)


# Spark Streaming

### Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams.

![image](https://user-images.githubusercontent.com/81246356/206632945-0dc23f47-b0ae-4493-9f69-ad937a611cc3.png)


### Internally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.
![image](https://user-images.githubusercontent.com/81246356/206633011-dd4d858c-9530-48dc-8c3c-62fd763889fd.png)


# Installing Spark
-  Download spark-2.3.2 to the local machine:
```
$ wget https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz
```
![img-1 install spark](https://user-images.githubusercontent.com/81246356/207105022-dbe3d08a-8b01-4663-bad2-0e1a8cda8755.jpg)

- Unpack
```
$ tar -xvf spark-3.3.1-bin-hadoop3.tgz
```
![img-2 install spark](https://user-images.githubusercontent.com/81246356/207105056-3d2d329c-37fa-45a7-abf5-e8be5e8db3dc.jpg)

- create a soft link:
```
ln -s /home/krisvesselee18/spark-3.3.1-bin-hadoop3 /home/krisvesselee18/spark
```
# Set Spark Environment Variable

## Add Entry to bashrc file
```
export SPARK_HOME=/home/dpandey/spark

export PATH=$SPARK_HOME/bin:$PATH

export PATH=$SPARK_HOME/sbin:$PATH
```

## Load bashrc file
```
source ~/.bashrc
```
# Install Java
```
sudo apt-get update
sudo apt install default-jdk
java --version
update-alternatives --list java
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/bin/java
```

# Verify Installation
```
pyspark
```
![img-5 install spark](https://user-images.githubusercontent.com/81246356/207106717-e7a83308-70a4-4beb-81f9-5aa79e3429b0.jpg)


### Start the master on local machine
- start-master.sh
- URL: http://34.70.113.82:8080/
![img-9 start master](https://user-images.githubusercontent.com/81246356/207110982-8b709593-5f0b-4d8e-9469-839c5899cf91.jpg)

![img-6 install spark](https://user-images.githubusercontent.com/81246356/207111037-644725a0-6da8-4886-b2a5-441ba9417ffe.jpg)
### Start worker
- start-slave.sh spark://34.70.113.82:7077
- browse URL: http://34.70.113.82:8080/

![img-7 install spark-slave](https://user-images.githubusercontent.com/81246356/207111274-aeceac67-e4d2-47bd-854f-833c545fad6e.jpg)

### RUn Word Count example in Python

- Open Terminal (1)
```
nc -lk 9999
```
![run nc 9999](https://user-images.githubusercontent.com/81246356/207112289-d94ca2d3-3d10-40ab-8613-e953809ad2a9.jpg)

- Open Terminal (2)
```
cd spark
./bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost 9999
```
### Expected Ouput

![word-count output](https://user-images.githubusercontent.com/81246356/207112846-d50065da-d466-489a-8a0a-8191b5895eff.jpg)

